
# train.py (Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ÙÙ‚Ø· ÛŒÚ© Ø¨Ø§Ø± Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§)
import os
import joblib
import numpy as np
import pandas as pd
import pandas_ta as ta
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

# ------------------------------------------------------------------
# Û±. ØªØ¹Ø±ÛŒÙ ØªÙˆØ§Ø¨Ø¹ Ù„Ø§Ø²Ù… (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡)
# ------------------------------------------------------------------

# Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø§ÛŒØ¯ Ù…Ù†Ø·Ù‚ Ú©Ø§Ù…Ù„ calculate_indicators_and_targets Ø´Ù…Ø§ Ø¨Ø§Ø´Ø¯.
# Ù…Ø§ Ø§Ø² ÛŒÚ© Ù†Ø³Ø®Ù‡ Ø³Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
def calculate_indicators_and_targets(df):
    df['Returns'] = df['close'].pct_change()
    df.ta.ema(length=20, append=True)
    df.ta.ema(length=50, append=True)
    df.ta.rsi(length=14, append=True)
    df.ta.atr(length=14, append=True)
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ ØªÙ…Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
    df['ADX'] = df.ta.adx(length=14).iloc[:, 0] # ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ†Ú©Ù‡ ADX Ø¯Ø±Ø³Øª Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÙˆØ¯
    df['Volatility'] = df['high'] - df['low']
    df['Hour'] = pd.to_datetime(df['datetime']).dt.hour
    df['DayOfWeek'] = pd.to_datetime(df['datetime']).dt.dayofweek
    df['HV_20'] = df['Returns'].rolling(window=20).std()
    
    df['RSI_14'] = df.get(next((c for c in df.columns if c.startswith('RSI_14')), ''), 0)
    df['RSI_6'] = df.ta.rsi(length=6) 
    df['EMA_Diff_Fast'] = df.get(next((c for c in df.columns if c.startswith('EMA_20')), ''), 0) - df.get(next((c for c in df.columns if c.startswith('EMA_50')), ''), 0)
    df['EMA_Diff_Slow'] = df.get(next((c for c in df.columns if c.startswith('EMA_50')), ''), 0) - df.ta.ema(length=100)
    
    # Ø§ÛŒØ¬Ø§Ø¯ ÛŒÚ© Target Ù†Ù…ÙˆÙ†Ù‡ (Ø§ÛŒÙ† Ø¨Ø§ÛŒØ¯ Target ÙˆØ§Ù‚Ø¹ÛŒ Ø´Ù…Ø§ Ø¨Ø§Ø´Ø¯)
    df['Target'] = (df['close'].shift(-5) > df['close']).astype(int)
    
    return df.dropna().reset_index(drop=True)

# ------------------------------------------------------------------
# Û². ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ùˆ Ø¢Ù…ÙˆØ²Ø´
# ------------------------------------------------------------------

print("1. Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡...")
# ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ
data = np.random.rand(2000, 4) * 100
df_sample = pd.DataFrame(data, columns=['open', 'high', 'low', 'close'])
df_sample['datetime'] = pd.to_datetime(pd.date_range('2024-01-01', periods=2000, freq='h'))

# Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡
df_processed = calculate_indicators_and_targets(df_sample)

# Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ
feature_cols = ['RSI_14', 'RSI_6', 'ADX', 'EMA_Diff_Fast', 'EMA_Diff_Slow', 'Returns', 'Volatility', 'Hour', 'DayOfWeek', 'HV_20']
X = df_processed[feature_cols].values
y = df_processed['Target'].values

# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# ğŸ”‘ Ø¢Ù…ÙˆØ²Ø´ StandardScaler Ùˆ Transform
print("2. Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Scaler...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# ------------------------------------------------------------------
# Û³. Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ 2D (RF, LR, XGB)
# ------------------------------------------------------------------

# RF
print("3. Ø¢Ù…ÙˆØ²Ø´ Random Forest Ùˆ Ø°Ø®ÛŒØ±Ù‡...")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)
joblib.dump(rf_model, 'models/rf_model.pkl')

# LR
print("4. Ø¢Ù…ÙˆØ²Ø´ Logistic Regression Ùˆ Ø°Ø®ÛŒØ±Ù‡...")
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train_scaled, y_train)
joblib.dump(lr_model, 'models/lr_model.pkl')

# XGB
print("5. Ø¢Ù…ÙˆØ²Ø´ XGBoost Ùˆ Ø°Ø®ÛŒØ±Ù‡...")
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train_scaled, y_train)
joblib.dump(xgb_model, 'models/xgb_model.pkl')


# ------------------------------------------------------------------
# Û´. Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ 3D (LSTM)
# ------------------------------------------------------------------

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ LSTM (3D: Samples, TimeSteps, Features)
def create_lstm_dataset(X, time_steps=10):
    Xs = []
    for i in range(len(X) - time_steps):
        v = X[i:(i + time_steps)]
        Xs.append(v)
    return np.array(Xs)

TIME_STEPS = 10 
X_train_lstm = create_lstm_dataset(X_train_scaled, TIME_STEPS)
y_train_lstm = y_train[TIME_STEPS:] # ØªØ·Ø§Ø¨Ù‚ Ø¯Ø§Ø¯Ù† Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§

print(f"6. Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ LSTM (TimeSteps={TIME_STEPS}) Ùˆ Ø°Ø®ÛŒØ±Ù‡...")
lstm_model = tf.keras.Sequential([
    tf.keras.layers.LSTM(units=50, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
lstm_model.compile(optimizer='adam', loss='binary_crossentropy')
lstm_model.fit(X_train_lstm, y_train_lstm, epochs=1, batch_size=32, verbose=0) 
lstm_model.save('models/lstm_model.h5')

# ------------------------------------------------------------------
# Ûµ. Ø°Ø®ÛŒØ±Ù‡ Scaler Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ù¾Ø§ÛŒØ§Ù†
# ------------------------------------------------------------------

# ğŸ’¡ Ø°Ø®ÛŒØ±Ù‡ Scaler Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ (Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù…!)
joblib.dump(scaler, 'models/scaler.pkl')

print("âœ… ØªÙ…Ø§Ù… Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Scaler Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ù¾ÙˆØ´Ù‡ models/ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
